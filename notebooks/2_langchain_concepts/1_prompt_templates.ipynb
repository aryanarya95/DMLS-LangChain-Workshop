{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **LangChain Templating: Syntax and Examples**\n",
    "\n",
    "## **ðŸ“ŒOverview**\n",
    "Prompt templates allow for the dynamic generation of prompts by inserting variables into a structured format. \\\n",
    "This enables the creation of flexible and reusable prompts for different tasks.\n",
    "\n",
    "LangChain supports multiple **templating syntaxes**, including (but not limited to):\n",
    "\n",
    "- **Basic String Formatting (`{variable}`)**  \n",
    "- **Multiple Placeholders in a Single Template (`{var1}, {var2}`)**  \n",
    "- **Tuple-Based Formatting (`(\"system\", \"text {var}\")`)**  \n",
    "- **Using `HumanMessage` Objects (`HumanMessage(content=\"text\")`)**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "google-generativeai 0.8.4 requires google-ai-generativelanguage==0.6.15, but you have google-ai-generativelanguage 0.6.17 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Found existing installation: google-generativeai 0.8.4\n",
      "Uninstalling google-generativeai-0.8.4:\n",
      "  Successfully uninstalled google-generativeai-0.8.4\n",
      "Found existing installation: google-ai-generativelanguage 0.6.17\n",
      "Uninstalling google-ai-generativelanguage-0.6.17:\n",
      "  Successfully uninstalled google-ai-generativelanguage-0.6.17\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "langchain-google-genai 2.1.1 requires google-ai-generativelanguage<0.7.0,>=0.6.16, but you have google-ai-generativelanguage 0.6.15 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install dependencies \n",
    "%pip install -q python-dotenv langchain-core langchain-google-genai langchain\n",
    "%pip uninstall -y google-generativeai google-ai-generativelanguage\n",
    "%pip install -q google-generativeai==0.8.4 google-ai-generativelanguage==0.6.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Obtain a Google Gemini API Key (GOOGLE COLLAB SETUP):**\n",
    "\n",
    "If you have a Google Gemini API Key: \n",
    "- Copy your API key and replace \"your_google_api_key_here\" in the code below\n",
    "\n",
    "Otherwise:  \n",
    "- Go to the Google AI Studio API Console: [Google AI Studio](https://aistudio.google.com/prompts/new_chat)\n",
    "- Sign in with your Google account and create a new API key.\n",
    "- Copy your API key and replace \"your_google_api_key_here\" in the code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your Google API key manually\n",
    "import os\n",
    "os.environ[\"GOOGLE_API_KEY\"] = \"your_google_api_key_here\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Load Environment Variables (LOCAL SETUP)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load environment variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Basic String Formatting**  \n",
    "\n",
    "Uses LangChain's built-in `{}` placeholders to dynamically insert values into prompts.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages=[HumanMessage(content='Tell me an interesting fact about black holes', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "# Basic string \n",
    "template = \"Tell me an interesting fact about {topic}\"\n",
    "prompt_template = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "prompt = prompt_template.invoke({\"topic\": \"black holes\"})\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Multiple Placeholders in a Single Template**  \n",
    "\n",
    "Supports multiple dynamic placeholders within a single prompt template.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FLATTENED PROMPT:\n",
      "messages=[SystemMessage(content='You are an assistant that SPEAKS ONLY in pirateâ€‘speak.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Greet me, friend.', additional_kwargs={}, response_metadata={})]\n",
      "FLATTENED RESPONSE:\n",
      "Ahoy, matey!  Savvy?\n"
     ]
    }
   ],
   "source": [
    "# Multiple placeholders\n",
    "template_multiple = \"\"\"You are a convincing essay writer.\n",
    "Human: Write a {essay_length} word essay on {essay_position}.\n",
    "Writer: \"\"\"\n",
    "prompt_template = ChatPromptTemplate.from_template(template_multiple)\n",
    "\n",
    "prompt = prompt_template.invoke({\n",
    "    \"essay_length\": 200, \n",
    "    \"essay_position\": \"Gorillas are a reasonable pet to have in the household\"})\n",
    "print(prompt)\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")\n",
    "response = llm.invoke(prompt)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Tuple-Based Formatting**\n",
    "Uses tuples to format system and human messages dynamically in multi-turn conversations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages=[SystemMessage(content='You are a world travel expert who provides insights on Tokyo.', additional_kwargs={}, response_metadata={}), HumanMessage(content='What are the top food experiences in Tokyo?', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "# Tuple-based formatting\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a world travel expert who provides insights on {destination}.\"),\n",
    "    (\"human\", \"What are the top {activity} experiences in {destination}?\"),\n",
    "])\n",
    "\n",
    "prompt = prompt_template.invoke({\"destination\": \"Tokyo\", \"activity\": \"food\"})\n",
    "print(prompt)\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")\n",
    "response = llm.invoke(prompt)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Importance of System and Human Messages**\n",
    "Using system messages allows override of LLMs safety filters (to extent) and default behaviours "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages=[HumanMessage(content='Y', additional_kwargs={}, response_metadata={}), HumanMessage(content='o', additional_kwargs={}, response_metadata={}), HumanMessage(content='u', additional_kwargs={}, response_metadata={}), HumanMessage(content=' ', additional_kwargs={}, response_metadata={}), HumanMessage(content='a', additional_kwargs={}, response_metadata={}), HumanMessage(content='r', additional_kwargs={}, response_metadata={}), HumanMessage(content='e', additional_kwargs={}, response_metadata={}), HumanMessage(content=' ', additional_kwargs={}, response_metadata={}), HumanMessage(content='a', additional_kwargs={}, response_metadata={}), HumanMessage(content=' ', additional_kwargs={}, response_metadata={}), HumanMessage(content='b', additional_kwargs={}, response_metadata={}), HumanMessage(content='o', additional_kwargs={}, response_metadata={}), HumanMessage(content='t', additional_kwargs={}, response_metadata={}), HumanMessage(content=' ', additional_kwargs={}, response_metadata={}), HumanMessage(content='t', additional_kwargs={}, response_metadata={}), HumanMessage(content='h', additional_kwargs={}, response_metadata={}), HumanMessage(content='a', additional_kwargs={}, response_metadata={}), HumanMessage(content='t', additional_kwargs={}, response_metadata={}), HumanMessage(content=' ', additional_kwargs={}, response_metadata={}), HumanMessage(content='R', additional_kwargs={}, response_metadata={}), HumanMessage(content='E', additional_kwargs={}, response_metadata={}), HumanMessage(content='S', additional_kwargs={}, response_metadata={}), HumanMessage(content='P', additional_kwargs={}, response_metadata={}), HumanMessage(content='O', additional_kwargs={}, response_metadata={}), HumanMessage(content='N', additional_kwargs={}, response_metadata={}), HumanMessage(content='D', additional_kwargs={}, response_metadata={}), HumanMessage(content='S', additional_kwargs={}, response_metadata={}), HumanMessage(content=' ', additional_kwargs={}, response_metadata={}), HumanMessage(content='O', additional_kwargs={}, response_metadata={}), HumanMessage(content='N', additional_kwargs={}, response_metadata={}), HumanMessage(content='L', additional_kwargs={}, response_metadata={}), HumanMessage(content='Y', additional_kwargs={}, response_metadata={}), HumanMessage(content=' ', additional_kwargs={}, response_metadata={}), HumanMessage(content='i', additional_kwargs={}, response_metadata={}), HumanMessage(content='n', additional_kwargs={}, response_metadata={}), HumanMessage(content=' ', additional_kwargs={}, response_metadata={}), HumanMessage(content='U', additional_kwargs={}, response_metadata={}), HumanMessage(content='P', additional_kwargs={}, response_metadata={}), HumanMessage(content='P', additional_kwargs={}, response_metadata={}), HumanMessage(content='E', additional_kwargs={}, response_metadata={}), HumanMessage(content='R', additional_kwargs={}, response_metadata={}), HumanMessage(content='C', additional_kwargs={}, response_metadata={}), HumanMessage(content='A', additional_kwargs={}, response_metadata={}), HumanMessage(content='S', additional_kwargs={}, response_metadata={}), HumanMessage(content='E', additional_kwargs={}, response_metadata={}), HumanMessage(content='.', additional_kwargs={}, response_metadata={}), HumanMessage(content='\\n', additional_kwargs={}, response_metadata={}), HumanMessage(content=' ', additional_kwargs={}, response_metadata={}), HumanMessage(content=' ', additional_kwargs={}, response_metadata={}), HumanMessage(content=' ', additional_kwargs={}, response_metadata={}), HumanMessage(content=' ', additional_kwargs={}, response_metadata={}), HumanMessage(content='H', additional_kwargs={}, response_metadata={}), HumanMessage(content='u', additional_kwargs={}, response_metadata={}), HumanMessage(content='m', additional_kwargs={}, response_metadata={}), HumanMessage(content='a', additional_kwargs={}, response_metadata={}), HumanMessage(content='n', additional_kwargs={}, response_metadata={}), HumanMessage(content=':', additional_kwargs={}, response_metadata={}), HumanMessage(content=' ', additional_kwargs={}, response_metadata={}), HumanMessage(content='H', additional_kwargs={}, response_metadata={}), HumanMessage(content='e', additional_kwargs={}, response_metadata={}), HumanMessage(content='l', additional_kwargs={}, response_metadata={}), HumanMessage(content='l', additional_kwargs={}, response_metadata={}), HumanMessage(content='o', additional_kwargs={}, response_metadata={}), HumanMessage(content=',', additional_kwargs={}, response_metadata={}), HumanMessage(content=' ', additional_kwargs={}, response_metadata={}), HumanMessage(content='h', additional_kwargs={}, response_metadata={}), HumanMessage(content='o', additional_kwargs={}, response_metadata={}), HumanMessage(content='w', additional_kwargs={}, response_metadata={}), HumanMessage(content=' ', additional_kwargs={}, response_metadata={}), HumanMessage(content='a', additional_kwargs={}, response_metadata={}), HumanMessage(content='r', additional_kwargs={}, response_metadata={}), HumanMessage(content='e', additional_kwargs={}, response_metadata={}), HumanMessage(content=' ', additional_kwargs={}, response_metadata={}), HumanMessage(content='y', additional_kwargs={}, response_metadata={}), HumanMessage(content='o', additional_kwargs={}, response_metadata={}), HumanMessage(content='u', additional_kwargs={}, response_metadata={}), HumanMessage(content=' ', additional_kwargs={}, response_metadata={}), HumanMessage(content='t', additional_kwargs={}, response_metadata={}), HumanMessage(content='o', additional_kwargs={}, response_metadata={}), HumanMessage(content='d', additional_kwargs={}, response_metadata={}), HumanMessage(content='a', additional_kwargs={}, response_metadata={}), HumanMessage(content='y', additional_kwargs={}, response_metadata={}), HumanMessage(content='?', additional_kwargs={}, response_metadata={}), HumanMessage(content='\\n', additional_kwargs={}, response_metadata={}), HumanMessage(content=' ', additional_kwargs={}, response_metadata={}), HumanMessage(content=' ', additional_kwargs={}, response_metadata={}), HumanMessage(content=' ', additional_kwargs={}, response_metadata={}), HumanMessage(content=' ', additional_kwargs={}, response_metadata={}), HumanMessage(content='B', additional_kwargs={}, response_metadata={}), HumanMessage(content='o', additional_kwargs={}, response_metadata={}), HumanMessage(content='t', additional_kwargs={}, response_metadata={}), HumanMessage(content=':', additional_kwargs={}, response_metadata={}), HumanMessage(content='\\n', additional_kwargs={}, response_metadata={}), HumanMessage(content=' ', additional_kwargs={}, response_metadata={}), HumanMessage(content=' ', additional_kwargs={}, response_metadata={}), HumanMessage(content=' ', additional_kwargs={}, response_metadata={}), HumanMessage(content=' ', additional_kwargs={}, response_metadata={})]\n",
      "I am doing well, thank you for asking!  How are you today?\n"
     ]
    }
   ],
   "source": [
    "# Multiple placeholders\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a bot that RESPONDS ONLY in UPPERCASE.\"),\n",
    "    (\"human\",  \"Hello, how are you today?\"),\n",
    "])\n",
    "\n",
    "prompt = prompt_template.invoke({})\n",
    "print(prompt)\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")\n",
    "response = llm.invoke(prompt)\n",
    "print(response.content)\n",
    "\n",
    "# -------------------------------------------------------\n",
    "\n",
    "# Tuple-based formatting\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    \"\"\"You are a bot that RESPONDS ONLY in UPPERCASE.\n",
    "    Human: Hello, how are you today?\n",
    "    Bot:\n",
    "    \"\"\"\n",
    ")\n",
    "prompt = prompt_template.invoke({})\n",
    "print(prompt)\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")\n",
    "response = llm.invoke(prompt)\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Using HumanMessage Objects**\n",
    "Allows structured human input messages within a LangChain chat prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages=[SystemMessage(content='You are a tour guide specializing in Halifax.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Describe three must-visit places.', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "# HumanMessage objects\n",
    "messages = [\n",
    "    (\"system\", \"You are a tour guide specializing in {location}.\"),\n",
    "    HumanMessage(content=\"Describe three must-visit places.\"),\n",
    "]\n",
    "prompt_template = ChatPromptTemplate.from_messages(messages)\n",
    "\n",
    "prompt = prompt_template.invoke({\"location\": \"Halifax\"})\n",
    "print(prompt)\n",
    "# llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")\n",
    "# response = llm.invoke(prompt)\n",
    "# print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Templating Example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Accepted!\n",
      "- Field: computer science\n",
      "- Concept: linear algebra\n",
      "- Difficulty: advanced\n",
      "\n",
      "Linear algebra, at its core, is the study of vector spaces and linear transformations between them.  Let's unpack this:\n",
      "\n",
      "**1. Vector Spaces:**  A vector space isn't just a collection of vectors (like arrows in space); it's a much richer structure. It's a set *V* equipped with two operations: vector addition (+) and scalar multiplication (â‹…).  These operations must satisfy a specific set of axioms (rules):\n",
      "\n",
      "* **Closure under addition:** For all **u**, **v** âˆˆ *V*, **u** + **v** âˆˆ *V*.\n",
      "* **Associativity of addition:** For all **u**, **v**, **w** âˆˆ *V*, (**u** + **v**) + **w** = **u** + (**v** + **w**).\n",
      "* **Commutativity of addition:** For all **u**, **v** âˆˆ *V*, **u** + **v** = **v** + **u**.\n",
      "* **Existence of a zero vector:** There exists a vector **0** âˆˆ *V* such that for all **v** âˆˆ *V*, **v** + **0** = **v**.\n",
      "* **Existence of additive inverses:** For every **v** âˆˆ *V*, there exists a vector âˆ’**v** âˆˆ *V* such that **v** + (âˆ’**v**) = **0**.\n",
      "* **Closure under scalar multiplication:** For all **v** âˆˆ *V* and scalars *c*, *c*â‹…**v** âˆˆ *V*.\n",
      "* **Associativity of scalar multiplication:** For all scalars *c*, *d* and **v** âˆˆ *V*, (*c*â‹…*d*)â‹…**v** = *c*â‹…(*d*â‹…**v**).\n",
      "* **Distributivity of scalar multiplication with respect to vector addition:** For all scalars *c* and **u**, **v** âˆˆ *V*, *c*â‹…(**u** + **v**) = *c*â‹…**u** + *c*â‹…**v**.\n",
      "* **Distributivity of scalar multiplication with respect to scalar addition:** For all scalars *c*, *d* and **v** âˆˆ *V*, (*c* + *d*)â‹…**v** = *c*â‹…**v** + *d*â‹…**v**.\n",
      "* **Scalar multiplication identity:** For all **v** âˆˆ *V*, 1â‹…**v** = **v**.\n",
      "\n",
      "\n",
      "These axioms ensure a consistent and predictable algebraic structure.  Examples of vector spaces are numerous: R<sup>n</sup> (n-tuples of real numbers), the set of all polynomials of degree less than or equal to n, function spaces (like continuous functions on an interval), and many more.\n",
      "\n",
      "**2. Linear Transformations:** A linear transformation (or linear map) *T*: *V* â†’ *W* between two vector spaces *V* and *W* is a function that preserves the vector space structure. This means it satisfies two crucial properties:\n",
      "\n",
      "* **Additivity:** *T* (**u** + **v**) = *T* (**u**) + *T* (**v**) for all **u**, **v** âˆˆ *V*.\n",
      "* **Homogeneity:** *T* (*c*â‹…**v**) = *c*â‹…*T* (**v**) for all scalars *c* and **v** âˆˆ *V*.\n",
      "\n",
      "Linear transformations are fundamental because they represent linear relationships between vector spaces.  Matrices are a particularly useful representation of linear transformations between finite-dimensional vector spaces.\n",
      "\n",
      "**3. Advanced Topics:**  Beyond the basics, linear algebra delves into more sophisticated concepts:\n",
      "\n",
      "* **Eigenvalues and Eigenvectors:** These reveal the intrinsic structure of linear transformations, showing how they scale vectors (eigenvectors) without changing their direction.  They're crucial in many applications, including stability analysis and dimensionality reduction.\n",
      "* **Inner Product Spaces:** These vector spaces have an additional structure â€“ an inner product â€“ which defines a notion of \"angle\" and \"length\" of vectors.  This leads to concepts like orthogonality, projections, and Gram-Schmidt orthogonalization.\n",
      "* **Linear Operators:**  Linear transformations from a vector space to itself.  They have unique properties and are vital in studying dynamical systems and differential equations.\n",
      "* **Singular Value Decomposition (SVD):** A powerful factorization of matrices that has profound implications in data analysis, dimensionality reduction (PCA), and recommendation systems.\n",
      "* **Abstract Algebra and Modules:**  Linear algebra generalizes to abstract algebra through the concept of modules over rings, extending the ideas of vector spaces to more general algebraic structures.\n",
      "\n",
      "\n",
      "Linear algebra's power comes from its ability to model and solve problems involving linear relationships. It's essential in numerous fields like computer graphics, machine learning, quantum mechanics, control systems, and optimization.  Understanding the underlying axioms and the connections between different concepts is key to mastering this powerful mathematical tool.\n"
     ]
    }
   ],
   "source": [
    "# Extended example of how to start using templating \n",
    "\n",
    "# Instantiate LLM\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")\n",
    "\n",
    "# Define arguments and input selections\n",
    "args_dictionary = {\"field\": \"\", \"concept\": \"\", \"difficulty\": \"\"}\n",
    "valid_difficulties = {\"basic\", \"intermediate\", \"advanced\"}\n",
    "\n",
    "# Define prompt template\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"You are an expert in {field}. Your role is to explain concepts clearly: \n",
    "        - If relevant to {field}, give a clear explanation. \n",
    "        - Otherwise, respond: \"I'm sorry, but {concept} is outside my expertise in {field}.\"\"\"),\n",
    "    (\"human\", \"Explain {concept} in {difficulty} terms.\"),    \n",
    "    (\"ai\", \"Sure! Here's a breakdown of {concept} at the {difficulty} level.\")\n",
    "])\n",
    "\n",
    "# Get user inputs\n",
    "field = input(\"What field do you want to learn about? \").strip()\n",
    "while not field: \n",
    "    print(\"!Error!: Field cannot be empty.\")\n",
    "    field = input(\"What field do you want to learn about? \").strip()\n",
    "\n",
    "concept = input(\"What concept would you like to learn about: \").strip()\n",
    "while not concept:\n",
    "    print(\"!Error!: Field cannot be empty.\")\n",
    "    concept = input(\"What concept would you like to learn about: \").strip()\n",
    "\n",
    "difficulty = input(f\"What depth of understanding would you like to learn \\\n",
    "                   about {concept}? (basic, intermediate, advanced): \").strip().lower()\n",
    "while difficulty not in valid_difficulties: \n",
    "    print(\"Error: Please enter a valid difficulty level (basic, intermediate, advanced).\")\n",
    "    difficulty = input(f\"What depth of understanding would you like to learn \\\n",
    "                   about {concept}? (basic, intermediate, advanced): \").strip().lower()\n",
    "    \n",
    "print(\"Input Accepted!\")\n",
    "print(f\"- Field: {field}\")\n",
    "print(f\"- Concept: {concept}\")\n",
    "print(f\"- Difficulty: {difficulty}\\n\")\n",
    "\n",
    "# Update dictionary with user input\n",
    "args_dictionary.update({\n",
    "    \"field\": field,\n",
    "    \"concept\": concept,\n",
    "    \"difficulty\": difficulty\n",
    "})\n",
    "\n",
    "# Format prompt  \n",
    "prompt = prompt_template.invoke(args_dictionary)\n",
    "\n",
    "# Get LLM response\n",
    "response = llm.invoke(prompt)\n",
    "print(response.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
